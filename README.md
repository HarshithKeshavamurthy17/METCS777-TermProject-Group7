# Wikipedia Clickstream Anomaly Detection System

**MET CS 777 Term Project - Group 7**

**Team Members:**
- Harshith Keshavamurthy
- Aryaman Jalali
- Dirgha Jivani

---

## üìã Table of Contents
- [Overview](#overview)
- [Project Structure](#project-structure)
- [Environment Setup](#environment-setup)
- [How to Run the Code](#how-to-run-the-code)
- [Dataset Description](#dataset-description)
- [Results](#results)
- [Troubleshooting](#troubleshooting)
- [Code Quality](#code-quality)

---

## üîç Overview

This project implements an **end-to-end Big Data pipeline** for detecting anomalies in Wikipedia clickstream data. The system processes large-scale clickstream logs (tens of GB) using Apache Spark to identify unusual patterns in user navigation.

### Anomaly Types Detected

1. **Traffic Spikes** üìà
   - Sudden, statistically significant increases in page transitions
   - Detection method: Robust Z-scores with MAD (Median Absolute Deviation)
   - Threshold: Z-score > 3.5 or deviation ratio > 10x baseline

2. **Mix-Shift Anomalies** üîÑ
   - Significant changes in the distribution of traffic sources for a page
   - Detection method: Referrer distribution analysis
   - Indicates changes in how users discover content

3. **Navigation-Edge Anomalies** üó∫Ô∏è
   - Unusual navigation patterns that deviate from established clusters
   - Detection method: K-means clustering on edge feature vectors
   - Identifies rare or anomalous user paths

---

## üìÅ Project Structure

```
METCS777-TermProject-Group7/
‚îú‚îÄ‚îÄ src/                          # Main source code
‚îÇ   ‚îú‚îÄ‚îÄ etl/                      # Data ingestion and cleaning
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clickstream_loader.py
‚îÇ   ‚îú‚îÄ‚îÄ features/                 # Feature engineering (baselines)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ baseline.py
‚îÇ   ‚îú‚îÄ‚îÄ detectors/                # Anomaly detection algorithms
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ statistical_detector.py    # Traffic spike detection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clustering_detector.py     # Navigation edge detection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mix_shift_detector.py      # Mix-shift detection
‚îÇ   ‚îú‚îÄ‚îÄ storage/                  # Data storage schemas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ anomalies_schema.py
‚îÇ   ‚îú‚îÄ‚îÄ pipeline/                 # Pipeline orchestration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ anomaly_detection_pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/                # Flask web application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ external/                 # External API integrations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pageviews_api.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/                    # Shared utilities
‚îÇ       ‚îú‚îÄ‚îÄ config.py
‚îÇ       ‚îî‚îÄ‚îÄ spark_session.py
‚îú‚îÄ‚îÄ scripts/                      # Executable scripts
‚îÇ   ‚îú‚îÄ‚îÄ download_clickstream.py   # Data downloader
‚îÇ   ‚îú‚îÄ‚îÄ run_detection.py          # Main Spark pipeline runner
‚îÇ   ‚îî‚îÄ‚îÄ start_dashboard_demo.py   # Dashboard server
‚îú‚îÄ‚îÄ config/                       # Configuration files
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml               # All pipeline parameters
‚îú‚îÄ‚îÄ data/                         # Data directory (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ raw/                      # Raw TSV files
‚îÇ   ‚îú‚îÄ‚îÄ processed/                # Processed Parquet files
‚îÇ   ‚îî‚îÄ‚îÄ anomalies/                # Detected anomalies
‚îú‚îÄ‚îÄ tests/                        # Unit tests
‚îú‚îÄ‚îÄ run_pipeline.py               # ‚≠ê Easy-to-run pipeline script
‚îú‚îÄ‚îÄ run_dashboard.py              # ‚≠ê Easy-to-run dashboard script
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îî‚îÄ‚îÄ README.md                     # Project documentation
```

---

## üõ†Ô∏è Environment Setup

### Prerequisites

Before you begin, ensure you have the following installed:

1. **Python 3.8+**
   ```bash
   python --version  # Should be 3.8 or higher
   ```

2. **Java 8 or 11** (Required for Apache Spark)
   ```bash
   java -version  # Should be 1.8 or 11
   ```
   
   - **macOS**: 
     ```bash
     brew install openjdk@11
     ```
   - **Ubuntu/Debian**:
     ```bash
     sudo apt-get install openjdk-11-jdk
     ```
   - **Windows**: Download from [Oracle](https://www.oracle.com/java/technologies/downloads/)

3. **System Requirements**
   - **RAM**: Minimum 8GB (16GB recommended for full dataset)
   - **Disk Space**: ~10GB for data and dependencies

### Installation Steps

#### Step 1: Clone the Repository

```bash
git clone https://github.com/HarshithKeshavamurthy17/METCS777-TermProject-Group7.git
cd METCS777-TermProject-Group7
```

#### Step 2: Create a Virtual Environment (Recommended)

```bash
# Create virtual environment
python -m venv venv

# Activate it
# On macOS/Linux:
source venv/bin/activate
# On Windows:
venv\Scripts\activate
```

#### Step 3: Install Python Dependencies

```bash
pip install -r requirements.txt
```

This will install:
- `pyspark>=3.5.0` - Apache Spark for distributed processing
- `flask>=3.0.0` - Web framework for dashboard
- `pandas>=2.0.0` - Data manipulation
- `numpy>=1.24.0` - Numerical computing
- `scipy>=1.10.0` - Scientific computing
- `requests>=2.31.0` - HTTP requests
- `pyyaml>=6.0` - Configuration file parsing

#### Step 4: Verify Installation

```bash
# Test Spark
python -c "from pyspark.sql import SparkSession; print('Spark OK')"

# Test other dependencies
python -c "import flask, pandas, numpy; print('All dependencies OK')"
```

---

## ‚ñ∂Ô∏è How to Run the Code

### Quick Start (3 Simple Steps)

#### 1Ô∏è‚É£ Download Data

```bash
python scripts/download_clickstream.py --months 2023-09 2023-10 2023-11 2023-12 2024-01 2024-02
```

**What this does:**
- Downloads Wikipedia clickstream TSV files from Wikimedia dumps
- Saves files to `data/raw/`
- Each file is ~1-2GB compressed

**Note:** This can take 10-30 minutes depending on your internet speed.

#### 2Ô∏è‚É£ Run the Pipeline

```bash
python run_pipeline.py
```

**What this does:**
1. **ETL Phase**: Loads and cleans TSV files
2. **Feature Engineering**: Calculates baselines (median, MAD, etc.)
3. **Anomaly Detection**: Runs all three detectors in parallel
4. **Storage**: Saves results to `data/anomalies/` in Parquet format

**Expected output:**
```
Starting Anomaly Detection Pipeline...
‚úì ETL completed: 10,234,567 edges processed
‚úì Baseline calculated: 1,567,890 unique edges
‚úì Statistical detector: 2,216 traffic spikes found
‚úì Mix-shift detector: 130 mix shifts found
‚úì Clustering detector: 0 navigation edges found
‚úì Total anomalies: 2,346
‚úì Saved to data/anomalies/
```

**Runtime:** ~10-20 minutes for 6 months of data (depends on your machine)

#### 3Ô∏è‚É£ Start the Dashboard

```bash
python run_dashboard.py
```

**What this does:**
- Launches a Flask web server on `http://localhost:5000`
- Loads all detected anomalies from `data/anomalies/`
- Provides interactive visualizations and filtering

**Expected output:**
```
Starting Dashboard...
‚úì Loaded 2,346 anomalies from partitioned parquet files
‚úì Months with anomalies: ['2023-09', '2023-10', '2023-11', '2023-12', '2024-01', '2024-02', '2024-03']
 * Running on http://127.0.0.1:5000
```

**Access the dashboard:**
Open your browser and navigate to: **http://localhost:5000**

---

## üìä Dataset Description

### Wikipedia Clickstream Data

The Wikipedia Clickstream dataset contains counts of (referrer, resource) pairs extracted from the request logs of Wikipedia.

**Source:** [Wikimedia Dumps - Clickstream](https://dumps.wikimedia.org/other/clickstream/)

**Format:** TSV (Tab-Separated Values)

**Schema:**
| Column | Type   | Description |
|--------|--------|-------------|
| `prev` | string | Referrer page (where the user came from) |
| `curr` | string | Current page (where the user navigated to) |
| `type` | string | Link type (e.g., `link`, `external`, `other`) |
| `n`    | int    | Number of transitions (count) |

**Example Row:**
```
Google	Main_Page	external	12453
```
This means 12,453 users navigated from Google to Wikipedia's Main Page.

**Dataset Size:**
- **Months Processed**: 6 months (Sep 2023 - Feb 2024)
- **Total Records**: ~10 million edges per month
- **Compressed Size**: ~1-2GB per month
- **Uncompressed Size**: ~5-10GB per month

---

## üìà Results

### Summary Statistics

Running the pipeline on 6 months of data (Sep 2023 - Feb 2024) produced:

| Metric | Value |
|--------|-------|
| **Total Anomalies Detected** | 2,346 |
| **Traffic Spikes** | 2,216 (94.5%) |
| **Mix-Shift Anomalies** | 130 (5.5%) |
| **Navigation Edges** | 0 (0%) |
| **Max Deviation Ratio** | 153.62x |
| **Avg Deviation Ratio** | 33.84x |

### Anomalies by Month

```
2023-09:  252 anomalies
2023-10:  543 anomalies
2023-11:  557 anomalies
2023-12:  251 anomalies
2024-01:  249 anomalies
2024-02:  249 anomalies
2024-03:  245 anomalies
```

### Example Anomalies

#### 1. Traffic Spike Example
```
Referrer: Main_Page
Target: 2023_Israel‚ÄìHamas_war
Month: 2023-10
Deviation Ratio: 153.62x
Z-Score: 45.2
```
**Interpretation:** This spike corresponds to the outbreak of the Israel-Hamas war in October 2023, causing massive traffic from Wikipedia's main page to the war article.

#### 2. Mix-Shift Example
```
Target Page: United_States
Month: 2023-11
Top Referrer Change: other-search ‚Üí Main_Page (+35%)
```
**Interpretation:** The United States page saw a significant shift in traffic sources, with more users arriving from the main page instead of search engines.

### Dashboard Features

The interactive dashboard provides:

1. **Overview Charts** üìä
   - Monthly anomaly counts by type
   - Trend analysis across time

2. **Filterable Anomaly Table** üîç
   - Filter by month, type, and deviation threshold
   - Sort by any column
   - Export capabilities

3. **Explainability Panel** üß†
   - Detailed breakdown of detection signals
   - Time-series charts showing traffic patterns
   - Referrer distribution changes
   - Z-score and deviation ratio visualizations

4. **Top Anomalies** üèÜ
   - Highlights the 5 most significant anomalies
   - Quick navigation to interesting cases

---

## üîß Troubleshooting

### Common Issues

#### Issue 1: "Out of Memory" Error

**Symptoms:**
```
java.lang.OutOfMemoryError: Java heap space
```

**Solution:**
Edit `config/config.yaml` and reduce memory allocation:
```yaml
spark:
  executor_memory: "4g"  # Reduce from 8g
  driver_memory: "4g"    # Reduce from 8g
```

Or process fewer months at a time.

---

#### Issue 2: "Port 5000 Already in Use"

**Symptoms:**
```
Address already in use
Port 5000 is in use by another program
```

**Solution:**
Run the dashboard on a different port:
```bash
PORT=5002 python run_dashboard.py
```

Then access via `http://localhost:5002`

---

#### Issue 3: "JAVA_HOME Not Set"

**Symptoms:**
```
Please set JAVA_HOME
```

**Solution:**
Set the JAVA_HOME environment variable:
```bash
# macOS/Linux (add to ~/.bashrc or ~/.zshrc)
export JAVA_HOME=$(/usr/libexec/java_home)

# Or manually:
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-11.jdk/Contents/Home
```

---

#### Issue 4: Download Script Fails

**Symptoms:**
```
Failed to download clickstream data
```

**Solution:**
Manually download files from:
https://dumps.wikimedia.org/other/clickstream/

Then place them in `data/raw/` with naming: `clickstream-YYYY-MM.tsv.gz`

---

## ‚úÖ Code Quality

### Design Principles

1. **Modularity** üß©
   - Clean separation between ETL, feature engineering, and detection
   - Each detector is independent and can be run separately
   - Easy to add new anomaly detection methods

2. **Configuration-Driven** ‚öôÔ∏è
   - All parameters in `config/config.yaml`
   - No hardcoded values
   - Easy to tune thresholds without code changes

3. **Scalability** üìà
   - Built on Apache Spark for distributed processing
   - Handles datasets exceeding local memory
   - Partitioned storage for efficient querying

4. **Documentation** üìù
   - Comprehensive docstrings in all modules
   - Inline comments explaining complex logic
   - Type hints for better code clarity

5. **Error Handling** üõ°Ô∏è
   - Graceful degradation when external APIs fail
   - Fallback mechanisms for missing data
   - Clear error messages with debugging guidance

### Code Comments

All code files include:
- Module-level docstrings explaining purpose
- Function/class docstrings with parameters and return types
- Inline comments for complex algorithms
- Examples in critical sections

---

## üéì Academic Context

**Course:** MET CS 777 - Big Data Analytics  
**Institution:** Boston University Metropolitan College  
**Semester:** Fall 2024  
**Submission Date:** December 2024

---

## üìß Contact

For questions or issues, please contact:
- Harshith Keshavamurthy
- Aryaman Jalali
- Dirgha Jivani

---

## üìÑ License

This project is submitted as part of academic coursework for MET CS 777.

---

**Happy Anomaly Hunting! üîç**
